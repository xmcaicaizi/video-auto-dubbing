# æ ‡å‡†IndexTTS 2.0çš„ä¿®æ”¹ä»£ç 

# ============================================
# ä¿®æ”¹ä½ç½®1ï¼šç¬¬338è¡Œå·¦å³çš„startup_eventå‡½æ•°
# ============================================

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶çš„åˆå§‹åŒ–"""
    global tts_model, args  # æ³¨æ„ï¼šéœ€è¦æ·»åŠ argsåˆ°global

    print("Starting IndexTTS VLLM API server...")
    print(f"Upload directory: {UPLOAD_DIR}")

    # æ¸…ç†æ—§æ–‡ä»¶
    await cleanup_old_files()

    # ğŸ”¥ IndexTTS 2.0æ ‡å‡†åŠ è½½æ–¹å¼
    try:
        # å¯¼å…¥IndexTTSæ¨¡å—
        from indextts.infer_v2 import IndexTTS2

        # åŠ è½½æ¨¡å‹
        tts_model = IndexTTS2(
            cfg_path=os.path.join(args.model_dir, "config.yaml"),
            model_dir=args.model_dir,
            use_fp16=True,  # æ ¹æ®ä½ çš„GPUæƒ…å†µè°ƒæ•´
            use_cuda_kernel=False,
            use_deepspeed=False
        )

        print(f"âœ… IndexTTS 2.0 model loaded from {args.model_dir}")

    except Exception as e:
        print(f"âŒ Failed to load IndexTTS model: {e}")
        print(f"Model directory: {args.model_dir}")
        tts_model = None

    print("Server startup complete.")

# ============================================
# ä¿®æ”¹ä½ç½®2ï¼šç¬¬165è¡Œå·¦å³çš„tts_inference_same_audio_refå‡½æ•°
# ============================================

async def tts_inference_same_audio_ref(text: str, spk_audio_path: str, emo_alpha: float, max_tokens: int):
    """ä½¿ç”¨åŒä¸€éŸ³é¢‘æ–‡ä»¶ä½œä¸ºéŸ³è‰²å’Œæƒ…æ„Ÿå‚è€ƒ"""
    try:
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        output_filename = f"tts_output_{int(time.time() * 1000)}.wav"
        output_path = f"/tmp/{output_filename}"

        # ğŸ”¥ ä½¿ç”¨IndexTTS 2.0å®˜æ–¹API
        if tts_model and hasattr(tts_model, 'infer'):
            result = tts_model.infer(
                spk_audio_prompt=spk_audio_path,    # éŸ³è‰²å‚è€ƒ
                emo_audio_prompt=spk_audio_path,    # æƒ…æ„Ÿå‚è€ƒï¼ˆåŒä¸€æ–‡ä»¶ï¼‰
                emo_alpha=emo_alpha,               # æƒ…æ„Ÿå¼ºåº¦
                text=text,
                output_path=output_path,
                max_text_tokens_per_sentence=max_tokens,
                verbose=True
            )

            print(f"âœ… TTS synthesis completed: {output_path}")
            return output_path
        else:
            raise Exception("TTS model not loaded or infer method not available")

    except Exception as e:
        print(f"âŒ TTS inference error: {e}")
        raise Exception(f"TTS inference failed: {str(e)}")

# ============================================
# ä¿®æ”¹ä½ç½®3ï¼šç¬¬200è¡Œå·¦å³çš„tts_inference_with_emotion_refå‡½æ•°
# ============================================

async def tts_inference_with_emotion_ref(text: str, spk_audio_path: str, emo_audio_path: str, emo_alpha: float, max_tokens: int):
    """ä½¿ç”¨ç‹¬ç«‹çš„éŸ³è‰²å’Œæƒ…æ„Ÿå‚è€ƒéŸ³é¢‘"""
    try:
        output_filename = f"tts_output_{int(time.time() * 1000)}.wav"
        output_path = f"/tmp/{output_filename}"

        if tts_model and hasattr(tts_model, 'infer'):
            result = tts_model.infer(
                spk_audio_prompt=spk_audio_path,    # éŸ³è‰²å‚è€ƒ
                emo_audio_prompt=emo_audio_path,    # ç‹¬ç«‹æƒ…æ„Ÿå‚è€ƒ
                emo_alpha=emo_alpha,               # æƒ…æ„Ÿå¼ºåº¦
                text=text,
                output_path=output_path,
                max_text_tokens_per_sentence=max_tokens,
                verbose=True
            )

            print(f"âœ… TTS synthesis with emotion completed: {output_path}")
            return output_path
        else:
            raise Exception("TTS model not loaded")

    except Exception as e:
        print(f"âŒ TTS inference with emotion error: {e}")
        raise Exception(f"TTS inference with emotion failed: {str(e)}")

# ============================================
# ä¿®æ”¹ä½ç½®4ï¼šmainå‡½æ•°ä¸­æ·»åŠ argsä¸ºå…¨å±€å˜é‡
# ============================================

def main():
    global args  # æ·»åŠ è¿™ä¸€è¡Œ

    parser = argparse.ArgumentParser(description="IndexTTS VLLM API Server with Voice Cloning")
    parser.add_argument("--model_dir", type=str, required=True, help="Path to model directory")
    parser.add_argument("--host", type=str, default="0.0.0.0", help="Host to bind to")
    parser.add_argument("--port", type=int, default=6006, help="Port to bind to")
    parser.add_argument("--gpu_memory_utilization", type=float, default=0.25, help="GPU memory utilization")

    args = parser.parse_args()

    print(f"Starting server with model_dir: {args.model_dir}")
    print(f"Server will be available at http://{args.host}:{args.port}")
    print(f"GPU memory utilization: {args.gpu_memory_utilization}")

    # å¯åŠ¨æœåŠ¡å™¨
    uvicorn.run(
        app,
        host=args.host,
        port=args.port,
        log_level="info"
    )